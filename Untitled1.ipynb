{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d946c4-b4ae-47de-91e5-8cea7d14d661",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCpp\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"E:\\\\rag\\\\models\\\\mistral-7b-v0.1.Q4_0.gguf\",\n",
    "    n_gpu_layers=0,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    ")\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "home  = home + '\\data'\n",
    "documents = SimpleDirectoryReader(home).load_data()\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "query_engine=index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\" generate 5 questions from the book\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001a082a-4eed-4ba4-ab90-a014ee5d78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b02d78-0842-454b-bc3d-476264b1ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952a6056-b905-4b5c-aebd-7053ecc21559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1eaf10f-0116-466c-a272-69f82d88ee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from E:\\rag\\models\\mistral-7b-v0.1.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"E:\\\\rag\\\\models\\\\mistral-7b-v0.1.Q4_0.gguf\",\n",
    "    n_gpu_layers=0,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf36c240-da6b-46ac-bc1f-943b111345ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5083.90 ms\n",
      "llama_print_timings:      sample time =      44.32 ms /   256 runs   (    0.17 ms per token,  5775.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5083.74 ms /     6 tokens (  847.29 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:        eval time =   50987.01 ms /   255 runs   (  199.95 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   56937.31 ms /   261 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuantum computers are a new kind of computer that can solve problems too complex to be solved by classical computers. They could revolutionize everything from cryptography and drug discovery, to weather forecasting and traffic modeling. They may even provide the basis for artificial intelligence.\\n\\nWhat exactly do they do?\\nA quantum computer is made up of qubits – basic units of information that are like binary bits in a classical computer. But unlike ordinary bits which can be either 0 or 1, qubits can be both 0 and 1 simultaneously. A single bit can store one bit of information (yes/no), whereas a single qubit can encode two states at once (yes and no).\\n\\nIn theory this means that quantum computers are exponentially faster than classical computers – so fast they could solve problems that would take thousands of years on current technology, in just hours. They have the potential to revolutionize every aspect of our lives: from medicine and energy production, to transportation and communications.\\n\\n## What is Quantum Computer\\n\\nQuantum computing is a field of science that aims to create a new kind of computer capable of solving problems too complex for today’s classical computers. This type of machine would use quantum physics principles such as super'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"what is quantam compute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a9f714-deb9-4d47-98fa-69fdc8720a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25378e71-67c6-45a4-9d2f-c53369624017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b789ab0c-731f-4ef0-b487-ff72f1c86050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aad704cc9bd4c5c9760d51f1a6697e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\rag\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rajukumar\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b758f2649a84b13879865d4d5127df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb10aabf064e109fa633ebac14cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a17c4f9f554924b36d4a0d10f442b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a4734e6c88402195da302c7cd73cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d21b1779ee4c308ed2929d0a18987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6cf532a15e48abaf3e8e4768bcb0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bdd14a9f354d89a80117d670f4e407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c14ecd45054c69916040acdfa72a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9174d93d9d3e4edf9147cb131d63d13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434876a8d00840f08fd172d1c2d52be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa98e119-eb15-41a5-a116-6d914d199659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7709e244-be94-4cb1-ab6d-12aaae799ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "home = os.getcwd()\n",
    "print(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42446fc-9a90-41a4-9826-044d8e7efeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "home  = home + '\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e8db6f-d0f0-4d7a-b6b6-7b749afb37be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\rag\\\\data'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4ba43d1-1cec-4519-a49c-132f9b294644",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(home).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1be78897-4446-4e20-943a-8c69cc05c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1c60dcf-a61a-49eb-9eda-b87c36588352",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6ac984-caf0-4113-a554-3d0a689483f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-langchain\n",
      "  Downloading llama_index_embeddings_langchain-0.1.2-py3-none-any.whl.metadata (663 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-embeddings-langchain) (0.10.27)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in e:\\rag\\myenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.3.1)\n",
      "Requirement already satisfied: httpx in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.16 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.1.16)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.8.1)\n",
      "Requirement already satisfied: numpy in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.2)\n",
      "Requirement already satisfied: pandas in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.0.3)\n",
      "Requirement already satisfied: pydantic>=1.10 in e:\\rag\\myenv\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.16->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.6.4)\n",
      "Requirement already satisfied: anyio in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.3.0)\n",
      "Requirement already satisfied: certifi in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.6)\n",
      "Requirement already satisfied: sniffio in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\rag\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\rag\\myenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\rag\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\rag\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\rag\\myenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.0.3)\n",
      "Requirement already satisfied: colorama in e:\\rag\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\rag\\myenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\rag\\myenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\rag\\myenv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in e:\\rag\\myenv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\rag\\myenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.16->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in e:\\rag\\myenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.16->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in e:\\rag\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
      "Downloading llama_index_embeddings_langchain-0.1.2-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: llama-index-embeddings-langchain\n",
      "Successfully installed llama-index-embeddings-langchain-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eed802da-56f3-4196-b891-c6a772b88ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d89d395-691d-4def-b8f8-b5bacd96a8aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    5083.90 ms\n",
      "llama_print_timings:      sample time =      17.78 ms /    54 runs   (    0.33 ms per token,  3036.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56109.46 ms /   476 tokens (  117.88 ms per token,     8.48 tokens per second)\n",
      "llama_print_timings:        eval time =   23266.58 ms /    53 runs   (  438.99 ms per token,     2.28 tokens per second)\n",
      "llama_print_timings:       total time =   79850.59 ms /   529 tokens\n"
     ]
    }
   ],
   "source": [
    " response = query_engine.query(\" generate 5 questions from the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eabc130e-66af-4090-918b-9cd12d829afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Which programming language is Python?\n",
      "2. What is a computer program?\n",
      "3. How do we write a computer program?\n",
      "4. Why do we write a computer program?\n",
      "5. When do we write a computer program?\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca4eab8e-28e9-45b6-bf26-1a46958a7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-langchain in e:\\rag\\myenv\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.3 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-llms-langchain) (0.1.14)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-llms-langchain) (0.10.27)\n",
      "Requirement already satisfied: llama-index-llms-anyscale<0.2.0,>=0.1.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-llms-langchain) (0.1.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-llms-langchain) (0.1.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.0.31)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.1.40)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.1.40)\n",
      "Requirement already satisfied: numpy<2,>=1 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in e:\\rag\\myenv\\lib\\site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (8.2.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2024.3.1)\n",
      "Requirement already satisfied: httpx in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.16 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.1.16)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.16.2)\n",
      "Requirement already satisfied: pandas in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (10.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in e:\\rag\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\rag\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\rag\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.21.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\rag\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in e:\\rag\\myenv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.37->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\rag\\myenv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.10.0)\n",
      "Requirement already satisfied: anyio in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (4.3.0)\n",
      "Requirement already satisfied: certifi in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (3.6)\n",
      "Requirement already satisfied: sniffio in e:\\rag\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\rag\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\rag\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\rag\\myenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\rag\\myenv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in e:\\rag\\myenv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\rag\\myenv\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\rag\\myenv\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in e:\\rag\\myenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.0.3)\n",
      "Requirement already satisfied: colorama in e:\\rag\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\rag\\myenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\rag\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\rag\\myenv\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\rag\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-langchain) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ed789-1805-46dd-98d9-46cd82b9346c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
